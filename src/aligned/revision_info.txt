arguments: align/align_dataset_mtcnn.py original aligned --image_size 182 --margin 44
--------------------
tensorflow version: 1.7.0
--------------------
git hash: b'99507a856f86460691cb2a27f1d5c3d4d2ee441d'
--------------------
b'diff --git a/src/align/align_dataset_mtcnn.py b/src/align/align_dataset_mtcnn.py\nindex 7d5e735..16abc78 100644\n--- a/src/align/align_dataset_mtcnn.py\n+++ b/src/align/align_dataset_mtcnn.py\n@@ -35,8 +35,10 @@ import facenet\n import align.detect_face\n import random\n from time import sleep\n+import time\n \n def main(args):\n+    tinit = time.time()\n     sleep(random.random())\n     output_dir = os.path.expanduser(args.output_dir)\n     if not os.path.exists(output_dir):\n@@ -92,8 +94,10 @@ def main(args):\n                         if img.ndim == 2:\n                             img = facenet.to_rgb(img)\n                         img = img[:,:,0:3]\n-    \n+                        t0 = time.time()\n                         bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n+                        t1 = time.time()\n+                        print("Total Time: " + str(t1 - t0))\n                         nrof_faces = bounding_boxes.shape[0]\n                         if nrof_faces>0:\n                             det = bounding_boxes[:,0:4]\n@@ -133,7 +137,8 @@ def main(args):\n                         else:\n                             print(\'Unable to align "%s"\' % image_path)\n                             text_file.write(\'%s\\n\' % (output_filename))\n-                            \n+    tfin = time.time()\n+    print("Total Time: " + str(tfin - tinit))\n     print(\'Total number of images: %d\' % nrof_images_total)\n     print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n             \ndiff --git a/src/classifier.py b/src/classifier.py\nindex 2adba82..b90f784 100644\n--- a/src/classifier.py\n+++ b/src/classifier.py\n@@ -1,19 +1,19 @@\n """An example of how to use your own dataset to train a classifier that recognizes people.\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -37,13 +37,14 @@ import pickle\n from sklearn.svm import SVC\n \n def main(args):\n-  \n+\n     with tf.Graph().as_default():\n-      \n+\n         with tf.Session() as sess:\n-            \n+\n+\n             np.random.seed(seed=args.seed)\n-            \n+\n             if args.use_split_dataset:\n                 dataset_tmp = facenet.get_dataset(args.data_dir)\n                 train_set, test_set = split_dataset(dataset_tmp, args.min_nrof_images_per_class, args.nrof_train_images_per_class)\n@@ -58,22 +59,22 @@ def main(args):\n             for cls in dataset:\n                 assert len(cls.image_paths)>0, \'There must be at least one image for each class in the dataset\'\n \n-                 \n+\n             paths, labels = facenet.get_image_paths_and_labels(dataset)\n-            \n+\n             print(\'Number of classes: %d\' % len(dataset))\n             print(\'Number of images: %d\' % len(paths))\n-            \n+\n             # Load the model\n             print(\'Loading feature extraction model\')\n             facenet.load_model(args.model)\n-            \n+\n             # Get input and output tensors\n             images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n             embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n             phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n             embedding_size = embeddings.get_shape()[1]\n-            \n+\n             # Run forward pass to calculate embeddings\n             print(\'Calculating features for images\')\n             nrof_images = len(paths)\n@@ -82,11 +83,14 @@ def main(args):\n             for i in range(nrof_batches_per_epoch):\n                 start_index = i*args.batch_size\n                 end_index = min((i+1)*args.batch_size, nrof_images)\n+                print(start_index,end_index)\n                 paths_batch = paths[start_index:end_index]\n                 images = facenet.load_data(paths_batch, False, False, args.image_size)\n                 feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n                 emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n-            \n+                file = open("original.txt","wb")\n+                file.write(emb_array.tobytes())\n+\n             classifier_filename_exp = os.path.expanduser(args.classifier_filename)\n \n             if (args.mode==\'TRAIN\'):\n@@ -94,7 +98,7 @@ def main(args):\n                 print(\'Training classifier\')\n                 model = SVC(kernel=\'linear\', probability=True)\n                 model.fit(emb_array, labels)\n-            \n+\n                 # Create a list of class names\n                 class_names = [ cls.name.replace(\'_\', \' \') for cls in dataset]\n \n@@ -102,7 +106,7 @@ def main(args):\n                 with open(classifier_filename_exp, \'wb\') as outfile:\n                     pickle.dump((model, class_names), outfile)\n                 print(\'Saved classifier model to file "%s"\' % classifier_filename_exp)\n-                \n+\n             elif (args.mode==\'CLASSIFY\'):\n                 # Classify images\n                 print(\'Testing classifier\')\n@@ -114,14 +118,17 @@ def main(args):\n                 predictions = model.predict_proba(emb_array)\n                 best_class_indices = np.argmax(predictions, axis=1)\n                 best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n-                \n+\n+                res = []\n                 for i in range(len(best_class_indices)):\n-                    print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n-                    \n-                accuracy = np.mean(np.equal(best_class_indices, labels))\n-                print(\'Accuracy: %.3f\' % accuracy)\n-                \n-            \n+                    print(\'%s\' % class_names[best_class_indices[i]])\n+                    print(\'%.3f\' % best_class_probabilities[i])\n+                #     print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n+                #\n+                # accuracy = np.mean(np.equal(best_class_indices, labels))\n+                # print(\'Accuracy: %.3f\' % accuracy)\n+\n+\n def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class):\n     train_set = []\n     test_set = []\n@@ -134,22 +141,22 @@ def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_clas\n             test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:]))\n     return train_set, test_set\n \n-            \n+\n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n+\n     parser.add_argument(\'mode\', type=str, choices=[\'TRAIN\', \'CLASSIFY\'],\n-        help=\'Indicates if a new classifier should be trained or a classification \' + \n+        help=\'Indicates if a new classifier should be trained or a classification \' +\n         \'model should be used for classification\', default=\'CLASSIFY\')\n     parser.add_argument(\'data_dir\', type=str,\n         help=\'Path to the data directory containing aligned LFW face patches.\')\n-    parser.add_argument(\'model\', type=str, \n+    parser.add_argument(\'model\', type=str,\n         help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'classifier_filename\', \n-        help=\'Classifier model file name as a pickle (.pkl) file. \' + \n+    parser.add_argument(\'classifier_filename\',\n+        help=\'Classifier model file name as a pickle (.pkl) file. \' +\n         \'For training this is the output and for classification this is an input.\')\n-    parser.add_argument(\'--use_split_dataset\', \n-        help=\'Indicates that the dataset specified by data_dir should be split into a training and test set. \' +  \n+    parser.add_argument(\'--use_split_dataset\',\n+        help=\'Indicates that the dataset specified by data_dir should be split into a training and test set. \' +\n         \'Otherwise a separate test set can be specified using the test_data_dir option.\', action=\'store_true\')\n     parser.add_argument(\'--test_data_dir\', type=str,\n         help=\'Path to the test data directory containing aligned images used for testing.\')\n@@ -163,7 +170,7 @@ def parse_arguments(argv):\n         help=\'Only include classes with at least this number of images in the dataset\', default=20)\n     parser.add_argument(\'--nrof_train_images_per_class\', type=int,\n         help=\'Use this number of images from each class for training and the rest for testing\', default=10)\n-    \n+\n     return parser.parse_args(argv)\n \n if __name__ == \'__main__\':\ndiff --git a/src/classify_faces.py b/src/classify_faces.py\nnew file mode 100644\nindex 0000000..57a6a38\n--- /dev/null\n+++ b/src/classify_faces.py\n@@ -0,0 +1,54 @@\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+import numpy as np\n+import facenet\n+import pickle\n+\n+\n+def classify_face(sess,graph,images_placeholder,embeddings,phase_train_placeholder,embedding_size):\n+    img_paths = ["aligned/somename/tmp.png"]\n+    pickle_file = "../../model/my_model.pkl"\n+    # model = "../../pretrained_model/"\n+\n+\n+    with graph.as_default():\n+\n+\n+        print("loaded model")\n+\n+        # images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n+        # embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n+        # phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+        # embedding_size = embeddings.get_shape()[1]\n+\n+        emb_array = np.zeros((1, embedding_size))\n+\n+        start_index = 0\n+        end_index = 1\n+\n+        images = facenet.load_data(img_paths, False, False, 160)\n+        feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n+        emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n+        file = open("mine.txt", "wb")\n+        file.write(emb_array.tobytes())\n+\n+        with open(pickle_file, \'rb\') as infile:\n+            (model, class_names) = pickle.load(infile)\n+\n+        print(\'Loaded classifier model from file "%s"\' % pickle_file)\n+\n+        predictions = model.predict_proba(emb_array)\n+        best_class_indices = np.argmax(predictions, axis=1)\n+        best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n+\n+        for i in range(len(best_class_indices)):\n+            # print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n+            return (class_names[best_class_indices[i]],best_class_probabilities)\n+        # accuracy = np.mean(np.equal(best_class_indices, labels))\n+        # print(\'Accuracy: %.3f\' % accuracy)\n+\n+\n+# classify_face()\n\\ No newline at end of file\ndiff --git a/src/freeze_graph.py b/src/freeze_graph.py\nindex 3584c18..36067a3 100644\n--- a/src/freeze_graph.py\n+++ b/src/freeze_graph.py\n@@ -1,7 +1,7 @@\n """Imports a model metagraph and checkpoint file, converts the variables to constants\n and exports the model as a graphdef protobuf\n """\n-# MIT License\n+# MIT Licensefres\n # \n # Copyright (c) 2016 David Sandberg\n # \n@@ -52,7 +52,7 @@ def main(args):\n             saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n             \n             # Retrieve the protobuf graph definition and fix the batch norm nodes\n-            input_graph_def = sess.graph.as_graph_def()\n+         input_graph_def = sess.graph.as_graph_def()\n             \n             # Freeze the graph def\n             output_graph_def = freeze_graph_def(sess, input_graph_def, \'embeddings,label_batch\')\ndiff --git a/src/recognize_faces.py b/src/recognize_faces.py\nnew file mode 100644\nindex 0000000..65594b6\n--- /dev/null\n+++ b/src/recognize_faces.py\n@@ -0,0 +1,89 @@\n+#use FLASK_APP=recognize_faces.py flask run\n+\n+from flask import Flask\n+from flask import request\n+import requests\n+import shutil\n+import os\n+import subprocess\n+import time\n+import tensorflow as tf\n+import facenet\n+import classify_faces\n+\n+app = Flask(__name__)\n+sess = None\n+\n+\n+@app.route("/")\n+def hello():\n+    return "Hello World!"\n+\n+\n+@app.route(\'/get_name\', methods=[\'GET\'])\n+def get_name():\n+\n+    t_init = time.time()\n+    print(sess)\n+\n+    url = request.args.get("url")\n+    os.system("rm -f aligned/somename/*")\n+    response = requests.get(url, stream=True)\n+    try:\n+        os.makedirs("original")\n+        os.makedirs("original/somename")\n+    except:\n+        pass\n+\n+    with open("original/somename/tmp.jpg", "wb") as f:\n+        shutil.copyfileobj(response.raw, f)\n+    os.system("python align/align_dataset_mtcnn.py original aligned --image_size 182 --margin 44")\n+    # os.system("rm -f aligned/somename/tmp.png")\n+\n+    t_fin = time.time()\n+    t0 = time.time()\n+    a,b = classify_faces.classify_face(sess,graph,images_placeholder,embeddings,phase_train_placeholder,embedding_size)\n+    # os.system("python classify_faces.py > result.txt")\n+    # os.system("python facenet/src/classifier.py \\\n+    # CLASSIFY \\\n+    # aligned/ \\\n+    # pretrained_model/ \\\n+    # model/my_model.pkl \\\n+    # --image_size 160 > result.txt")\n+\n+    t1 = time.time()\n+    # print(a)\n+    # print(b)\n+    print("Total time: ", str(t1-t0))\n+\n+    # file = open("result.txt","r")\n+    # line = file.readlines()\n+\n+    # res = line[len(line) - 2] + line[len(line) - 1]\n+    # return res\n+\n+    res = "Name: " + str(a) + "\\n" + \\\n+        "probability of it being correct: " + str(b[0]) + "\\n" + \\\n+        "time for classification: %0.3f seconds\\n" % (t1-t0) + \\\n+        "time for face alignment: %0.3f seconds\\n" % (t_fin-t_init)\n+\n+    return res\n+\n+if __name__ == \'__main__\':\n+    currentdir = os.getcwd()\n+    pythonpath = currentdir ##+ "/facenet/src"\n+    os.environ["PYTHONPATH"] = pythonpath\n+    model = "../../pretrained_model/"\n+\n+    graph = tf.Graph()\n+\n+    # with graph.as_default():\n+    with tf.Session() as sess:\n+\n+        facenet.load_model(model)\n+        images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n+        embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n+        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+        embedding_size = embeddings.get_shape()[1]\n+        print("loaded")\n+        app.run(debug=False)\n\\ No newline at end of file'